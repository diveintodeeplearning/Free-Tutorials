{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DIDL_Style_Transfer_Tutorial.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sV6bzP8_1WpI"
      },
      "source": [
        "# Neural style transfer\n",
        "\n",
        "Using deep learning, we are going to transfer the style of famous painters like Van Gogh to your own pics. Deep Art or neural style transfer is a great way to style your photos. \n",
        "\n",
        "This tutorial uses the pre-trained Arbitrary Image Stylization model from TensorFlow Hub.\n",
        "\n",
        "This algorithm optimizes each image to get the style you want. So, we have two images: the content image and the style image(like a Picasso painting). This algorithm blends both images together so the output image looks like the content image but with the style of Picasso (or another famous artist). \n",
        "\n",
        "This algorithm optimizes the output image to match the statistics of both images (content and style images). We use a convolutional network (CNN) to get these statistics. \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xrqqQqCa7c2-"
      },
      "source": [
        "# Setup\n",
        "\n",
        "First, we are going to import some libraries like *TensorFlow, IPython, matplotlib, numpy, PIL, time,* and *functools*. \n",
        "\n",
        "*TensorFlow* is used to make machine learning models. \n",
        "\n",
        "*Numpy* is a library used to do matrix math. \n",
        "\n",
        "*Ipython.display* is a library to display graphs. \n",
        "\n",
        "*Matplotlib* is a library to show graphs. \n",
        "\n",
        "Note: As you know, every image is a numerical matrix that represents each pixel. So, we use the *Numpy* library to manipulate images with matrix math. In fact, *machine learning* consists of matrix multiplications. \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B3MvBS7U1XY3"
      },
      "source": [
        "import tensorflow as tf\n",
        "import IPython.display as display\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib as mpl\n",
        "mpl.rcParams['figure.figsize'] = (12,12)\n",
        "mpl.rcParams['axes.grid'] = False\n",
        "\n",
        "import numpy as np\n",
        "import PIL.Image\n",
        "import time\n",
        "import functools\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gv_vZTeq-r_3"
      },
      "source": [
        "In machine learning, we use tensors to represent images. The next function receives a tensor and returns an image. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uZwvrV_21XVD"
      },
      "source": [
        "def tensor_to_image(tensor):\n",
        "  tensor = tensor*255\n",
        "  tensor = np.array(tensor, dtype=np.uint8)\n",
        "  if np.ndim(tensor)>3:\n",
        "    assert tensor.shape[0] == 1\n",
        "    tensor = tensor[0]\n",
        "  return PIL.Image.fromarray(tensor)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Plpii138_D20"
      },
      "source": [
        "Now, we are going to download our content and style image. \n",
        "\n",
        "You can use your own images. You just have to save it in the content folder. Go to **Files** (Folder icon) and then go to the **content** folder. Then, you have to upload your images. We recommend you to save your images as *content.jpg* and *style.jpg*. For this step, you must have to comment on the first two lines and uncomment the last two lines.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SVIFdQ6B_Ekn"
      },
      "source": [
        "content_path = tf.keras.utils.get_file('YellowLabradorLooking_new.jpg', 'https://storage.googleapis.com/download.tensorflow.org/example_images/YellowLabradorLooking_new.jpg')\n",
        "style_path = tf.keras.utils.get_file('kandinsky5.jpg','https://storage.googleapis.com/download.tensorflow.org/example_images/Vassily_Kandinsky%2C_1913_-_Composition_7.jpg')\n",
        "\n",
        "#content_path=\"content.jpg\"\n",
        "#style_path=\"style.jpg\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GUnXKRFJKJ6N"
      },
      "source": [
        "Now, we are going to define a function to load an image and limit its maximum dimension to 512 pixels. \n",
        "First, we define that the maximum dimension (max_dim) is 512 pixels. Then, we use *tf.io.reade_file* to load the image. Then, we decode the image, which means that the image has three channels (RGB). The color images have three layers (3 numerical matrices) that represent the RGB (Red, Blue, and Green) channels. Then we are going to convert (cast) the image to *tf.float32* type. \n",
        "\n",
        "Then, we are going to save the shape of the image in the shape variable. Then we are going to define some variables (like *shape, long_dim,* and *scale*) to reshape (or *resize*) our images and avoid shape problems later in our CNN model. \n",
        "\n",
        "Then, we reshape the image and return it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WxOysiEHKKHF"
      },
      "source": [
        "def load_img(path_to_img):\n",
        "  max_dim = 512\n",
        "  img = tf.io.read_file(path_to_img)\n",
        "  img = tf.image.decode_image(img, channels=3)\n",
        "  img = tf.image.convert_image_dtype(img, tf.float32)\n",
        "\n",
        "  shape = tf.cast(tf.shape(img)[:-1], tf.float32)\n",
        "  long_dim = max(shape)\n",
        "  scale = max_dim / long_dim\n",
        "\n",
        "  new_shape = tf.cast(shape * scale, tf.int32)\n",
        "\n",
        "  img = tf.image.resize(img, new_shape)\n",
        "  img = img[tf.newaxis, :]\n",
        "  return img"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "74AdTfm9NWJ4"
      },
      "source": [
        "Now, we define a simple function to display an image. For this, we are using *matplotlib* library and its *imshow* function. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nqWlu7j7NXAM"
      },
      "source": [
        "def imshow(image, title=None):\n",
        "  if len(image.shape) > 3:\n",
        "    image = tf.squeeze(image, axis=0)\n",
        "\n",
        "  plt.imshow(image)\n",
        "  if title:\n",
        "    plt.title(title)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eSBpknKEzseN"
      },
      "source": [
        "Now, we are using the load_img function to load both images: *content_image* and *style_image*.\n",
        "\n",
        "Then, we are going to show these images using the *subplot* function of the *matplotlib* library. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5_j1tGKmzsuV"
      },
      "source": [
        "content_image = load_img(content_path)\n",
        "style_image = load_img(style_path)\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "imshow(content_image, 'Content Image')\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "imshow(style_image, 'Style Image')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V2K4LnvP0GWy"
      },
      "source": [
        "## Fast Style Transfer using TF-Hub\n",
        "\n",
        "Now we are going to create our first styled image. We are going to use *TF-Hub*, which is a module for fast artistic style transfer. \n",
        "\n",
        "First, we are going to use the *TensorFlow-hub* library. Then, we are going to load the hub module. Then, we are going to use the *hub_model*, the *content*, and *style* images to create a *stylized_image*.\n",
        "\n",
        "Note: we used the *tensor_to_image* function to show the *stylized_image*. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7euy3oq80GhY"
      },
      "source": [
        "import tensorflow_hub as hub\n",
        "hub_model = hub.load('https://tfhub.dev/google/magenta/arbitrary-image-stylization-v1-256/2')\n",
        "stylized_image = hub_model(tf.constant(content_image), tf.constant(style_image))[0]\n",
        "tensor_to_image(stylized_image)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jkVbYny55Am8"
      },
      "source": [
        "Now, we are going to use a pre-trained model called *VGG19*. This pre-trained *Keras* model is a *deep learning* model with pre-trained weights that are often used for prediction, feature extraction, and fine-tuning. \n",
        "\n",
        "In this tutorial, we are going to use the intermediate layers of our model to get the content and style representations of the image.\n",
        "\n",
        "The first few layer activations (starting from the input layer) represent low-level features like edges and textures.   \n",
        "\n",
        "The final few layers represent higher-level features-object parts like eyes (in case of a face image). \n",
        "\n",
        "In style transfer, we are going to use this pre-trained image classification network (*VGG19*). The intermediate layers are used to define the representation of content and style from the images. We are going to use this model to try to match the style and content target representations.\n",
        "\n",
        "First, we are going to preprocess the content image and resize it (to avoid shape problems). Then, we are going to load the *VGG19* model in the *vgg* variable. Then, we are going to feed the *vgg* model with the pre-processed content image (*x*). Then, we are going to print its shape. \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AXSd6tv65AxS"
      },
      "source": [
        "x = tf.keras.applications.vgg19.preprocess_input(content_image*255)\n",
        "x = tf.image.resize(x, (224, 224))\n",
        "vgg = tf.keras.applications.VGG19(include_top=True, weights='imagenet')\n",
        "prediction_probabilities = vgg(x)\n",
        "prediction_probabilities.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lKOxew98-mZB"
      },
      "source": [
        "Now, we are going to show the top 5 classification predictions of this content image using the *VGG19* model.  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PxbX3IhL-mkL"
      },
      "source": [
        "predicted_top_5 = tf.keras.applications.vgg19.decode_predictions(prediction_probabilities.numpy())[0]\n",
        "[(class_name, prob) for (number, class_name, prob) in predicted_top_5]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tEQfEAyn_BJd"
      },
      "source": [
        "Now, we are going to load the *VGG19* model without the classification head, and list the layer names"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hFK7QNA9_FYp"
      },
      "source": [
        "vgg = tf.keras.applications.VGG19(include_top=False, weights='imagenet')\n",
        "\n",
        "print()\n",
        "for layer in vgg.layers:\n",
        "  print(layer.name)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VFgMinczBF9T"
      },
      "source": [
        "Now, we are going to choose intermediate layers from the *VGG19* model to represent the style and content of the image. Then, we are going to define the number of content layers and the number of style layers. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lrOYiZ2NBGLC"
      },
      "source": [
        "content_layers = ['block5_conv2'] \n",
        "\n",
        "style_layers = ['block1_conv1',\n",
        "                'block2_conv1',\n",
        "                'block3_conv1', \n",
        "                'block4_conv1', \n",
        "                'block5_conv1']\n",
        "\n",
        "num_content_layers = len(content_layers)\n",
        "num_style_layers = len(style_layers)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zydMZ054BkQS"
      },
      "source": [
        "#### Intermediate layers for style and content:\n",
        "\n",
        "As you know, we are using the intermediate outputs within the *VGG19* model to define style and content representation.\n",
        "\n",
        "A Convolutional Neural Network (*CNN*) is used for *image classification*. This is possible because *CNN* is able to understand the image. The *CNN* model is feed with the image as input pixels to convert it into a complex understanding of the image features.  \n",
        "\n",
        "A *CNN* is able to capture the invariances and defining features within classes (like cats and dogs) that has a noisy background. *CNN* is a feature extractor. We are going to use the pre-trained *CNN* to describe the content and style images. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jDgynenaKJKZ"
      },
      "source": [
        "### Build the model \n",
        "\n",
        "In this tutorial, we are using the *VGG19* model to do a fast-style-transfer model. When we use a pre-trained model, we have the option to only use some pre-trained layers. For this, we extract the intermediate layer values using Keras functional *API*. \n",
        "\n",
        "\n",
        "To define a model using *Keras* functional *API*, specify the inputs and outputs:\n",
        "\n",
        "`model = Model(inputs, outputs)`\n",
        "\n",
        "The next function builds a *VGG19* model that returns a list of intermediate layer outputs. First, it defines the *VGG19* model without the input layer and its pre-trained weights. Then, this model is set to not be trainable, which means that we are going to freeze the network (weights) before compiling the model. Then, we are going to get the intermediate layer outputs. Then, we are going to define the model using the *VGG19* input and its intermediate layer outputs. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H7SgMwmeKIY7"
      },
      "source": [
        "def vgg_layers(layer_names):\n",
        "  \"\"\" Creates a vgg model that returns a list of intermediate output values.\"\"\"\n",
        "  # Load our model. Load pretrained VGG, trained on imagenet data\n",
        "  vgg = tf.keras.applications.VGG19(include_top=False, weights='imagenet')\n",
        "  vgg.trainable = False\n",
        "  \n",
        "  outputs = [vgg.get_layer(name).output for name in layer_names]\n",
        "\n",
        "  model = tf.keras.Model([vgg.input], outputs)\n",
        "  return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zhKERZY4KIsL"
      },
      "source": [
        "Now, we are going to create the model.\n",
        "\n",
        "First, we are going to use the *vgg_layers* function to create a *vgg* model. Then, we are going to feed the *style_extractor* model with *style_image* to get the *style_outputs*. Then, we are going to look at the statistics of each layer's output. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9AIpqWfhBka5"
      },
      "source": [
        "style_extractor = vgg_layers(style_layers)\n",
        "style_outputs = style_extractor(style_image*255)\n",
        "\n",
        "#Look at the statistics of each layer's output\n",
        "for name, output in zip(style_layers, style_outputs):\n",
        "  print(name)\n",
        "  print(\"  shape: \", output.numpy().shape)\n",
        "  print(\"  min: \", output.numpy().min())\n",
        "  print(\"  max: \", output.numpy().max())\n",
        "  print(\"  mean: \", output.numpy().mean())\n",
        "  print()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ibnQE3ApVZS-"
      },
      "source": [
        "## Calculate style\n",
        "\n",
        "The content of an image is represented by the values of the intermediate feature maps.\n",
        "\n",
        "\n",
        "To calculate style, we are going to use the correlation and mean formulas. We use the correlation formula to know if one chunk of the image is correlated to another chunk. By doing this math, we easily know how much one chunk of the image is related to another chunk. \n",
        "\n",
        "For this, we calculate a Gram matrix that includes this information by taking the outer product of the feature vector with itself at each location and averaging that outer product over all locations. This Gram matrix can be calculated for a particular layer as:\n",
        "\n",
        "$$G^l_{cd} = \\frac{\\sum_{ij} F^l_{ijc}(x)F^l_{ijd}(x)}{IJ}$$\n",
        "\n",
        "This can be implemented concisely using the `tf.linalg.einsum` function:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dfR0Y2oVVaQf"
      },
      "source": [
        "def gram_matrix(input_tensor):\n",
        "  result = tf.linalg.einsum('bijc,bijd->bcd', input_tensor, input_tensor)\n",
        "  input_shape = tf.shape(input_tensor)\n",
        "  num_locations = tf.cast(input_shape[1]*input_shape[2], tf.float32)\n",
        "  return result/(num_locations)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2JRizv9VYxMp"
      },
      "source": [
        "## Extract style and content\n",
        "Now, we are going to build a model that returns the style and content tensors. For this, we are going to define the *StyleContentModel* class that receives the model to get the style. \n",
        "\n",
        "First, it defines the *init* function to initialize the variables that we are going to use such as *vgg* (model), *style_layers*, *content_layers*, *num_style_layers*. Also, we are going to freeze the network.\n",
        "\n",
        "Then, we are going to define the class *call* . First, we define the inputs, processed_input (for *VGG19* model), *outputs* (of the *VGG19* model), *style_outputs*, and *content_outputs*. \n",
        "\n",
        "To define style_outputs, we are going to use *gram_matrix* function. \n",
        "\n",
        "Then, we are going to define a *content_dict* that maps *content_layers* to *content_outputs*. \n",
        "\n",
        "Then, we are going to define a *style_dict* that maps *style_layers* to *style_outputs*.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QH6zIPjKJRDO"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jyAaajZTY10o"
      },
      "source": [
        "class StyleContentModel(tf.keras.models.Model):\n",
        "  def __init__(self, style_layers, content_layers):\n",
        "    super(StyleContentModel, self).__init__()\n",
        "    self.vgg =  vgg_layers(style_layers + content_layers)\n",
        "    self.style_layers = style_layers\n",
        "    self.content_layers = content_layers\n",
        "    self.num_style_layers = len(style_layers)\n",
        "    self.vgg.trainable = False\n",
        "\n",
        "  def call(self, inputs):\n",
        "    \"Expects float input in [0,1]\"\n",
        "    inputs = inputs*255.0\n",
        "    preprocessed_input = tf.keras.applications.vgg19.preprocess_input(inputs)\n",
        "    outputs = self.vgg(preprocessed_input)\n",
        "    style_outputs, content_outputs = (outputs[:self.num_style_layers], \n",
        "                                      outputs[self.num_style_layers:])\n",
        "\n",
        "    style_outputs = [gram_matrix(style_output)\n",
        "                     for style_output in style_outputs]\n",
        "\n",
        "    content_dict = {content_name:value \n",
        "                    for content_name, value \n",
        "                    in zip(self.content_layers, content_outputs)}\n",
        "\n",
        "    style_dict = {style_name:value\n",
        "                  for style_name, value\n",
        "                  in zip(self.style_layers, style_outputs)}\n",
        "    \n",
        "    return {'content':content_dict, 'style':style_dict}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-YKQxIVXMAGR"
      },
      "source": [
        "When called on an image, this model returns the *gram matrix (style)* of the *style_layers* and content of the *content_layers*.\n",
        "\n",
        "Now, we are going to define the *extractor* variable using *StyleContentModel* function, and feed it using *content_image* variable.\n",
        "\n",
        "Then, we are going to look at the statistics of the styles and contents. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ro-5qodUMARB"
      },
      "source": [
        "extractor = StyleContentModel(style_layers, content_layers)\n",
        "\n",
        "results = extractor(tf.constant(content_image))\n",
        "\n",
        "print('Styles:')\n",
        "for name, output in sorted(results['style'].items()):\n",
        "  print(\"  \", name)\n",
        "  print(\"    shape: \", output.numpy().shape)\n",
        "  print(\"    min: \", output.numpy().min())\n",
        "  print(\"    max: \", output.numpy().max())\n",
        "  print(\"    mean: \", output.numpy().mean())\n",
        "  print()\n",
        "\n",
        "print(\"Contents:\")\n",
        "for name, output in sorted(results['content'].items()):\n",
        "  print(\"  \", name)\n",
        "  print(\"    shape: \", output.numpy().shape)\n",
        "  print(\"    min: \", output.numpy().min())\n",
        "  print(\"    max: \", output.numpy().max())\n",
        "  print(\"    mean: \", output.numpy().mean())\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0FDdooVtNStS"
      },
      "source": [
        "## Run gradient descent\n",
        "\n",
        "Now, are are going to implement the style transfer algorithm using the *extractor* function. Do this by calculating the *mean square error* for your image's output relative to each target, then take the weighted sum of these losses.\n",
        "\n",
        "First, we are going to set your style and content target values using the *extractor*."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QSOvpzBNNTY8"
      },
      "source": [
        "style_targets = extractor(style_image)['style']\n",
        "content_targets = extractor(content_image)['content']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gKlPEAP2SNnL"
      },
      "source": [
        "Now, we are going to define the *image* variable, which is the generated image to be optimized. This *image* variable has the same shape as the *content* image, so we are using it to define the *image* variable. \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zcooRFhkSOND"
      },
      "source": [
        "image = tf.Variable(content_image)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NgufBU1ZXKww"
      },
      "source": [
        "Now, we are going to define the *clip_0_1* function. In this function, we are going to use the *clip_by_value* function to keep the pixel values between 0 and 1."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KiHSjrf9XLVw"
      },
      "source": [
        "def clip_0_1(image):\n",
        "  return tf.clip_by_value(image, clip_value_min=0.0, clip_value_max=1.0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T59SjO1UYCN4"
      },
      "source": [
        "Now, we are going to create an optimizer using the `Adam` optimizer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XMHnRKXxYCZ9"
      },
      "source": [
        "opt = tf.optimizers.Adam(learning_rate=0.02, beta_1=0.99, epsilon=1e-1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4sU_TrRFYaN5"
      },
      "source": [
        "To optimize this, use a weighted combination of the two losses (style and content) to get the total loss.\n",
        "\n",
        "First, we are going to define the *style_weight* and *content_weight*.\n",
        "\n",
        "Then, we are going to define the *style_content_loss* function to get the total loss using the outputs of the network. In this function, we have to define the *style_outputs* and *content_outputs*. Then, we use the *mean-square-error* formula for both losses. Then, we add both losses. \n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HMBx9-x-YadG"
      },
      "source": [
        "style_weight=1e-2\n",
        "content_weight=1e4"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NUofnY0QYfIy"
      },
      "source": [
        "def style_content_loss(outputs):\n",
        "    style_outputs = outputs['style']\n",
        "    content_outputs = outputs['content']\n",
        "    style_loss = tf.add_n([tf.reduce_mean((style_outputs[name]-style_targets[name])**2) \n",
        "                           for name in style_outputs.keys()])\n",
        "    style_loss *= style_weight / num_style_layers\n",
        "\n",
        "    content_loss = tf.add_n([tf.reduce_mean((content_outputs[name]-content_targets[name])**2) \n",
        "                             for name in content_outputs.keys()])\n",
        "    content_loss *= content_weight / num_content_layers\n",
        "    loss = style_loss + content_loss\n",
        "    return loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eiA39ZrUcHWc"
      },
      "source": [
        "Now, we are going to define the train_step function to update the image in each training step. \n",
        "\n",
        "For this, we use the *tf.GradientTape* function. First, we use the function extractor to describe the image. Then, we get the loss. Then, we compute and update gradients. Then, we use the *clip_0_1* function to keep pixel values between 0 and 1. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3KwAYHLjcHgi"
      },
      "source": [
        "@tf.function()\n",
        "def train_step(image):\n",
        "  with tf.GradientTape() as tape:\n",
        "    outputs = extractor(image)\n",
        "    loss = style_content_loss(outputs)\n",
        "\n",
        "  grad = tape.gradient(loss, image)\n",
        "  opt.apply_gradients([(grad, image)])\n",
        "  image.assign(clip_0_1(image))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6yc2itcYmbCO"
      },
      "source": [
        "Now, we are going to run a few training steps to test this function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3ISEd3fOmbRL"
      },
      "source": [
        "train_step(image)\n",
        "train_step(image)\n",
        "train_step(image)\n",
        "tensor_to_image(image)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6QAoVVxdmm36"
      },
      "source": [
        "Now, we are going to perform longer optimization.\n",
        "\n",
        "We are going to measure the optimization time. For this, we are going to import *time* library. Then, we are going to define the *start* time. \n",
        "\n",
        "Now, we are going to define the training hyperparameters such as *epochs*, *steps_per_epoch*, and *step*. The number of epochs is the number of times we are going to train the network. \n",
        "\n",
        "We are going to do a for loop to train the network *n* (*epochs*) times. Then, we are going to do a nested for loop to do *m* *(steps_per_epoch)* steps per epoch. In this nested loop, we are using the *train_step* function to do a training step and display the generated image.\n",
        "\n",
        "In the end, we are going to take the time optimization time. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ukAfwT2wmnA_"
      },
      "source": [
        "import time\n",
        "start = time.time()\n",
        "\n",
        "epochs = 10\n",
        "steps_per_epoch = 100\n",
        "\n",
        "step = 0\n",
        "for n in range(epochs):\n",
        "  for m in range(steps_per_epoch):\n",
        "    step += 1\n",
        "    train_step(image)\n",
        "    print(\".\", end='')\n",
        "  display.clear_output(wait=True)\n",
        "  display.display(tensor_to_image(image))\n",
        "  print(\"Train step: {}\".format(step))\n",
        "  \n",
        "end = time.time()\n",
        "print(\"Total time: {:.1f}\".format(end-start))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wq4dULkLN8KD"
      },
      "source": [
        "## Total variation loss\n",
        "\n",
        "Using the last implementation, we have a high-frequency-artifacts problem. To decrease this problem, we are going to use an explicit regularization term on the high-frequency components of the image. This is called *total variation loss*.\n",
        "\n",
        "For this, we are going to define the *high_pass_x_y* function, where we are going to add the regularization term to the image. This function returns a horizontal delta and a vertical delta. This function is like an edge detector of its horizontal and vertical edges.  \n",
        " "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CkWVxW1fN8du"
      },
      "source": [
        "def high_pass_x_y(image):\n",
        "  x_var = image[:,:,1:,:] - image[:,:,:-1,:]\n",
        "  y_var = image[:,1:,:,:] - image[:,:-1,:,:]\n",
        "\n",
        "  return x_var, y_var"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yn86KlsKODp5"
      },
      "source": [
        "x_deltas, y_deltas = high_pass_x_y(content_image)\n",
        "\n",
        "plt.figure(figsize=(14,10))\n",
        "plt.subplot(2,2,1)\n",
        "imshow(clip_0_1(2*y_deltas+0.5), \"Horizontal Deltas: Original\")\n",
        "\n",
        "plt.subplot(2,2,2)\n",
        "imshow(clip_0_1(2*x_deltas+0.5), \"Vertical Deltas: Original\")\n",
        "\n",
        "x_deltas, y_deltas = high_pass_x_y(image)\n",
        "\n",
        "plt.subplot(2,2,3)\n",
        "imshow(clip_0_1(2*y_deltas+0.5), \"Horizontal Deltas: Styled\")\n",
        "\n",
        "plt.subplot(2,2,4)\n",
        "imshow(clip_0_1(2*x_deltas+0.5), \"Vertical Deltas: Styled\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bxDI56CySYHn"
      },
      "source": [
        "The regularization loss associated with this is the sum of the squares of the values.\n",
        "\n",
        "Now, we are going to define the *total_variation_loss* function. First, we are going to get the horizontal and vertical edges (*x_deltas* and *y_deltas*), then we are going to add the *reduce_sum* of these values. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a9zKcDBISYTe"
      },
      "source": [
        "def total_variation_loss(image):\n",
        "  x_deltas, y_deltas = high_pass_x_y(image)\n",
        "  return tf.reduce_sum(tf.abs(x_deltas)) + tf.reduce_sum(tf.abs(y_deltas))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fEDAhICfUElZ"
      },
      "source": [
        "total_variation_loss(image).numpy()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Aqjd8CMdT_OU"
      },
      "source": [
        "That demonstrated what it does. \n",
        "\n",
        "But there's no need to implement it yourself. TensorFlow has its own total variation loss function called *total_variation*. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KNN2QODiT_Zz"
      },
      "source": [
        "tf.image.total_variation(image).numpy()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x6XjRKdlUhiZ"
      },
      "source": [
        "## Re-run the optimization\n",
        "\n",
        "Now, we are going to re-run the optimization using *tf.image.total_variation* function. First, we choose a weight for the `total_variation_loss`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rxCxy4lZUjr3"
      },
      "source": [
        "total_variation_weight=30"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JYKbBxxvUtdu"
      },
      "source": [
        "Now, we are going to update the *train_step* adding the weighted total variation loss to the *loss* variable. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rf1QlCdgVnmI"
      },
      "source": [
        "@tf.function()\n",
        "def train_step(image):\n",
        "  with tf.GradientTape() as tape:\n",
        "    outputs = extractor(image)\n",
        "    loss = style_content_loss(outputs)\n",
        "    loss += total_variation_weight*tf.image.total_variation(image)\n",
        "\n",
        "  grad = tape.gradient(loss, image)\n",
        "  opt.apply_gradients([(grad, image)])\n",
        "  image.assign(clip_0_1(image))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L-WOEpk3VwMU"
      },
      "source": [
        "Reinitialize the optimization variable, which has the content_image shape:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cAp1RmkyVwVJ"
      },
      "source": [
        "image = tf.Variable(content_image)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B_hIBH5FV5-i"
      },
      "source": [
        "Run the optimization:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-tzITcdFV6KM"
      },
      "source": [
        "import time\n",
        "start = time.time()\n",
        "\n",
        "epochs = 10\n",
        "steps_per_epoch = 100\n",
        "\n",
        "step = 0\n",
        "for n in range(epochs):\n",
        "  for m in range(steps_per_epoch):\n",
        "    step += 1\n",
        "    train_step(image)\n",
        "    print(\".\", end='')\n",
        "  display.clear_output(wait=True)\n",
        "  display.display(tensor_to_image(image))\n",
        "  print(\"Train step: {}\".format(step))\n",
        "\n",
        "end = time.time()\n",
        "print(\"Total time: {:.1f}\".format(end-start))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UN4LCZgKWDJl"
      },
      "source": [
        "Finally, save the result:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1B8E3EQEWDVU"
      },
      "source": [
        "file_name = 'stylized-image.png'\n",
        "tensor_to_image(image).save(file_name)\n",
        "\n",
        "try:\n",
        "  from google.colab import files\n",
        "except ImportError:\n",
        "   pass\n",
        "else:\n",
        "  files.download(file_name)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}